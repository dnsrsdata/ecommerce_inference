{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta fase, irei iniciar a etapa de experimentação, onde irei testar uma série \n",
    "de modelos de classificação para encontrar o que melhor se adequa ao problema.\n",
    "Vale ressaltar que apenas algumas colunas serão utilizadas para a criação dos\n",
    "modelos, pois algumas só são obtidas ao fim do processo do SISU, como é o caso\n",
    "da coluna \"NOTA_CORTE\" e \"CLASSIFICACAO\". Outras colunas como códigos das IES\n",
    "e cursos também não serão utilizadas, pois não são relevantes para o problema.\n",
    "Ao final, além do modelo, um conjunto de dados no formato .db será gerado para\n",
    "ser consumido pelo app final. Segue abaixo as colunas que serão utilizadas:\n",
    "\n",
    "- Modelo: IES, UF_CAMPUS, MUNICIPIO_CAMPUS, NOME_CURSO, GRAU, TURNO, \n",
    "TIPO_MOD_CONCORRENCIA, QT_VAGAS_CONCORRENCIA, PERCENTUAL_BONUS, PESO_L, PESO_CH,\n",
    "PESO_CN, PESO_M, PESO_R, NOTA_MINIMA_L, NOTA_MINIMA_CH, NOTA_MINIMA_CN, \n",
    "NOTA_MINIMA_M, NOTA_MINIMA_R, MEDIA_MINIMA, OPCAO, NOTA_L, NOTA_CH, NOTA_CN, \n",
    "NOTA_M, NOTA_R, NOTA_L_COM_PESO, NOTA_CH_COM_PESO, NOTA_CN_COM_PESO, \n",
    "NOTA_M_COM_PESO, NOTA_R_COM_PESO, NOTA_CANDIDATO e APROVADO.\n",
    "\n",
    "Vale ressaltar que parte das informações que serão utilizadas no Web App serão \n",
    "buscadas nos dados do SISU, como é o caso da QT_VAGAS_CONCORRENCIA, que é um \n",
    "valor que a universidade define para cada curso e não o usuário. Outras serão \n",
    "calculadas manualmente, como no caso das notas com peso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import joblib\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "\n",
    "# Preprocessing & Models\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "\n",
    "# Métricas\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo os dados\n",
    "dados_sisu_full = pd.read_parquet('../data/processed/dados_transformados.parquet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo as colunas que serão utilizadas para treinar o modelo\n",
    "colunas_para_buscar = ['IES', 'UF_CAMPUS', 'MUNICIPIO_CAMPUS', 'NOME_CURSO', \n",
    "                       'GRAU', 'TURNO', 'TIPO_MOD_CONCORRENCIA', \n",
    "                       'QT_VAGAS_CONCORRENCIA', 'PERCENTUAL_BONUS', 'PESO_L', \n",
    "                       'PESO_CH', 'PESO_CN', 'PESO_M', 'PESO_R', \n",
    "                       'NOTA_MINIMA_L', 'NOTA_MINIMA_CH', 'NOTA_MINIMA_CN', \n",
    "                       'NOTA_MINIMA_M', 'NOTA_MINIMA_R', 'MEDIA_MINIMA', \n",
    "                       'OPCAO', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', \n",
    "                       'NOTA_R', 'NOTA_L_COM_PESO', 'NOTA_CH_COM_PESO', \n",
    "                       'NOTA_CN_COM_PESO', 'NOTA_M_COM_PESO', 'NOTA_R_COM_PESO',\n",
    "                       'NOTA_CANDIDATO', 'APROVADO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrando as colunas\n",
    "dados_sisu = dados_sisu_full[colunas_para_buscar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N    0.898245\n",
       "S    0.101755\n",
       "Name: APROVADO, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando o balanceamento da variável alvo\n",
    "dados_sisu['APROVADO'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/daniel/Documents/sisu_analysis/notebooks/../mlruns/980913714926035010', creation_time=1702469991297, experiment_id='980913714926035010', last_update_time=1702469991297, lifecycle_stage='active', name='Comparando modelos', tags={}>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define o local para salvar os experimentos\n",
    "mlflow.set_tracking_uri('../mlruns')\n",
    "\n",
    "# Criando/acessando o experimento\n",
    "mlflow.set_experiment('Comparando modelos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividindo os dados em variaveis explicativas e variavel alvo\n",
    "x = dados_sisu.drop(columns=['APROVADO'])\n",
    "y = dados_sisu['APROVADO'].map({'S': 1, 'N': 0})\n",
    "\n",
    "# Dividindo os dados em treino e teste\n",
    "x_treino, x_teste, y_treino, y_teste = train_test_split(x, y, test_size=0.45, random_state=42, stratify=y)\n",
    "\n",
    "# Dividindo os dados em teste e dev\n",
    "x_teste, x_dev, y_teste, y_dev = train_test_split(x_teste, y_teste, test_size=0.5, random_state=42, stratify=y_teste)\n",
    "\n",
    "# Dividindo os dados em dev e calibração\n",
    "x_dev, x_calib, y_dev, y_calib = train_test_split(x_dev, y_dev, test_size=0.5, random_state=42, stratify=y_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um scaler padrão\n",
    "scale = y_treino.value_counts()[0] / y_treino.value_counts()[1] \n",
    "\n",
    "# Criando dicionário com os modelos\n",
    "dict_models_scale_sensitive_cw = {\"LR\": LogisticRegression(random_state=200, \n",
    "                                                           class_weight='balanced')}\n",
    "\n",
    "dict_models_scale_sensitive_no_cw = {\"LR\": LogisticRegression(random_state=200)}\n",
    "\n",
    "dict_models_tree_based_cw = {\"LGBM\": LGBMClassifier(is_unbalance=True,\n",
    "                                                 random_state=200),\n",
    "                          \"XGB\": XGBClassifier(scale_pos_weight=scale,\n",
    "                                               random_state=200),\n",
    "                          \"CTBC\": CatBoostClassifier(auto_class_weights='Balanced',\n",
    "                                                     random_state=200)}\n",
    "\n",
    "dict_models_tree_based_no_cw = {\"LGBM\": LGBMClassifier(random_state=200),\n",
    "                          \"XGB\": XGBClassifier(random_state=200),\n",
    "                          \"CTBC\": CatBoostClassifier(random_state=200)}\n",
    "\n",
    "# Criando dicionário com os encoders\n",
    "dict_encoders = {\"OHE\": OneHotEncoder(drop='first'),\n",
    "                 \"TE\": ce.TargetEncoder(),\n",
    "                 \"BE\": ce.BinaryEncoder(),\n",
    "                 \"ME\": ce.MEstimateEncoder(),\n",
    "                 \"WOE\": ce.WOEEncoder(),\n",
    "                 \"CE\": ce.CatBoostEncoder(),\n",
    "                 \"GE\":ce.GrayEncoder()}\n",
    "\n",
    "dict_scalers = {\"SS\": StandardScaler()}\n",
    "\n",
    "# Criando dicionário com os transformers\n",
    "dict_transformers = {\"PT\": PowerTransformer()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo as folds\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=200)\n",
    "\n",
    "# Retornando os nomes das colunas com mais de 25 valores únicos\n",
    "cat_cols = x_treino.select_dtypes(include='object').columns\n",
    "high_dim_cols = cat_cols[x_treino[cat_cols].nunique() > 25]\n",
    "\n",
    "# Retornando os nomes das colunas com menos de 25 valores únicos\n",
    "cat_cols = [col for col in cat_cols if col not in high_dim_cols]\n",
    "\n",
    "# Buscando as colunas numéricas\n",
    "num_cols = x_treino.select_dtypes(include=['int', 'float']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Iniciando os experimentos sem transformers e com class_weight\n",
    "#for tag, model in dict_models_scale_sensitive_cw.items():\n",
    "#    for tag_encoder, encoder in dict_encoders.items():\n",
    "#        for tag_scaler, scaler in dict_scalers.items():\n",
    "#            \n",
    "#            # Gerando a tag de identificação do modelo\n",
    "#            nome_modelo = f'{tag}_CW_{tag_encoder}_{tag_scaler}'\n",
    "#            \n",
    "#            with mlflow.start_run(run_name=nome_modelo):\n",
    "#                 \n",
    "#                 # Criando os pipeline com os transformers\n",
    "#                 pipe_cat = Pipeline([('encoder', encoder)])\n",
    "#                 pipe_high_dim = Pipeline([('encoder', ce.CountEncoder())])\n",
    "#                 pipe_num = Pipeline([('scaler', scaler)])\n",
    "#                 \n",
    "#                 # Criando o transformador\n",
    "#                 transformer = ColumnTransformer([('cat', pipe_cat, cat_cols),\n",
    "#                                                 ('num', pipe_num, num_cols),\n",
    "#                                                 ('high_dim', pipe_high_dim, high_dim_cols)])\n",
    "#                 \n",
    "#                 # Criando o pipeline final\n",
    "#                 pipe = Pipeline([('transformer', transformer),\n",
    "#                                 ('model', model)])\n",
    "#                 \n",
    "#                 # Executando o cross validation\n",
    "#                 cross_val_scores = cross_val_score(pipe, x_treino, y_treino, cv=kf, scoring='neg_log_loss')\n",
    "#                 \n",
    "#                 # Calculando a média das métricas\n",
    "#                 mean_score = cross_val_scores.mean()           \n",
    "#                 \n",
    "#                 # Salvando a métrica da folder 1\n",
    "#                 mlflow.log_metric('log_loss_fold_1', cross_val_scores[0])\n",
    "#                 \n",
    "#                 # Salvando a métrica da folder 2\n",
    "#                 mlflow.log_metric('log_loss_fold_2', cross_val_scores[1])\n",
    "#                \n",
    "#                 # Salvando a métrica da folder 3\n",
    "#                 mlflow.log_metric('log_loss_fold_3', cross_val_scores[2])\n",
    "#                \n",
    "#                 # Salvando a métrica da folder 4\n",
    "#                 mlflow.log_metric('log_loss_fold_4', cross_val_scores[3])\n",
    "#                \n",
    "#                 # Salvando a métrica da folder 5\n",
    "#                 mlflow.log_metric('log_loss_fold_5', cross_val_scores[4])\n",
    "#                 \n",
    "#                 # Salvando as métricas\n",
    "#                 mlflow.log_metric('log_loss_mean', mean_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Iniciando os experimentos sem transformers e sem class_weight\n",
    "#for tag, model in dict_models_scale_sensitive_no_cw.items():\n",
    "#    for tag_encoder, encoder in dict_encoders.items():\n",
    "#        for tag_scaler, scaler in dict_scalers.items():\n",
    "#            \n",
    "#            # Gerando a tag de identificação do modelo\n",
    "#            nome_modelo = f'{tag}_NO_CW_{tag_encoder}_{tag_scaler}'\n",
    "#            \n",
    "#            with mlflow.start_run(run_name=nome_modelo):\n",
    "#                 \n",
    "#                 # Criando os pipeline com os transformers\n",
    "#                 pipe_cat = Pipeline([('encoder', encoder)])\n",
    "#                 pipe_high_dim = Pipeline([('encoder', ce.CountEncoder())])\n",
    "#                 pipe_num = Pipeline([('scaler', scaler)])\n",
    "#                 \n",
    "#                 # Criando o transformador\n",
    "#                 transformer = ColumnTransformer([('cat', pipe_cat, cat_cols),\n",
    "#                                                 ('num', pipe_num, num_cols),\n",
    "#                                                 ('high_dim', pipe_high_dim, high_dim_cols)])\n",
    "#                 \n",
    "#                 # Criando o pipeline final\n",
    "#                 pipe = Pipeline([('transformer', transformer),\n",
    "#                                 ('model', model)])\n",
    "#                 \n",
    "#                 # Executando o cross validation\n",
    "#                 cross_val_scores = cross_val_score(pipe, x_treino, y_treino, cv=kf, scoring='neg_log_loss')\n",
    "#                 \n",
    "#                 # Calculando a média das métricas\n",
    "#                 mean_score = cross_val_scores.mean()           \n",
    "#                 \n",
    "#                 # Salvando a métrica da folder 1\n",
    "#                 mlflow.log_metric('log_loss_fold_1', cross_val_scores[0])\n",
    "#                 \n",
    "#                 # Salvando a métrica da folder 2\n",
    "#                 mlflow.log_metric('log_loss_fold_2', cross_val_scores[1])\n",
    "#                \n",
    "#                 # Salvando a métrica da folder 3\n",
    "#                 mlflow.log_metric('log_loss_fold_3', cross_val_scores[2])\n",
    "#                \n",
    "#                 # Salvando a métrica da folder 4\n",
    "#                 mlflow.log_metric('log_loss_fold_4', cross_val_scores[3])\n",
    "#                \n",
    "#                 # Salvando a métrica da folder 5\n",
    "#                 mlflow.log_metric('log_loss_fold_5', cross_val_scores[4])\n",
    "#                 \n",
    "#                 # Salvando as métricas\n",
    "#                 mlflow.log_metric('log_loss_mean', mean_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Iniciando os experimentos com transformers e sem class_weight\n",
    "#for tag, model in dict_models_scale_sensitive_no_cw.items():\n",
    "#    for tag_encoder, encoder in dict_encoders.items():\n",
    "#        for tag_scaler, scaler in dict_scalers.items():\n",
    "#            for tag_transformer, transformer in dict_transformers.items():\n",
    "#            \n",
    "#                # Gerando a tag de identificação do modelo\n",
    "#                nome_modelo = f'{tag}_NO_CW_{tag_encoder}_{tag_scaler}_{tag_transformer}'\n",
    "#\n",
    "#                with mlflow.start_run(run_name=nome_modelo):\n",
    "#\n",
    "#                     # Criando os pipeline com os transformers\n",
    "#                     pipe_cat = Pipeline([('encoder', encoder)])\n",
    "#                     pipe_high_dim = Pipeline([('encoder', ce.CountEncoder())])\n",
    "#                     pipe_num = Pipeline([('scaler', scaler),\n",
    "#                                          ('transformer', transformer)])\n",
    "#\n",
    "#                     # Criando o transformador\n",
    "#                     transformer = ColumnTransformer([('cat', pipe_cat, cat_cols),\n",
    "#                                                     ('num', pipe_num, num_cols),\n",
    "#                                                     ('high_dim', pipe_high_dim, high_dim_cols)])\n",
    "#\n",
    "#                     # Criando o pipeline final\n",
    "#                     pipe = Pipeline([('transformer', transformer),\n",
    "#                                     ('model', model)])\n",
    "#\n",
    "#                     # Executando o cross validation\n",
    "#                     cross_val_scores = cross_val_score(pipe, x_treino, y_treino, cv=kf, scoring='neg_log_loss')\n",
    "#\n",
    "#                     # Calculando a média das métricas\n",
    "#                     mean_score = cross_val_scores.mean()         \n",
    "#\n",
    "#                     # Salvando a métrica da folder 1\n",
    "#                     mlflow.log_metric('log_loss_fold_1', cross_val_scores[0])\n",
    "#\n",
    "#                     # Salvando a métrica da folder 2\n",
    "#                     mlflow.log_metric('log_loss_fold_2', cross_val_scores[1])\n",
    "#\n",
    "#                     # Salvando a métrica da folder 3\n",
    "#                     mlflow.log_metric('log_loss_fold_3', cross_val_scores[2])\n",
    "#\n",
    "#                     # Salvando a métrica da folder 4\n",
    "#                     mlflow.log_metric('log_loss_fold_4', cross_val_scores[3])\n",
    "#\n",
    "#                     # Salvando a métrica da folder 5\n",
    "#                     mlflow.log_metric('log_loss_fold_5', cross_val_scores[4])\n",
    "#\n",
    "#                     # Salvando as métricas\n",
    "#                     mlflow.log_metric('log_loss_mean', mean_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Iniciando os experimentos com transformers e com class_weight\n",
    "#for tag, model in dict_models_scale_sensitive_cw.items():\n",
    "#    for tag_encoder, encoder in dict_encoders.items():\n",
    "#        for tag_scaler, scaler in dict_scalers.items():\n",
    "#            for tag_transformer, transformer in dict_transformers.items():\n",
    "#            \n",
    "#                # Gerando a tag de identificação do modelo\n",
    "#                nome_modelo = f'{tag}_CW_{tag_encoder}_{tag_scaler}_{tag_transformer}'\n",
    "#\n",
    "#                with mlflow.start_run(run_name=nome_modelo):\n",
    "#\n",
    "#                     # Criando os pipeline com os transformers\n",
    "#                     pipe_cat = Pipeline([('encoder', encoder)])\n",
    "#                     pipe_high_dim = Pipeline([('encoder', ce.CountEncoder())])\n",
    "#                     pipe_num = Pipeline([('scaler', scaler),\n",
    "#                                          ('transformer', transformer)])\n",
    "#\n",
    "#                     # Criando o transformador\n",
    "#                     transformer = ColumnTransformer([('cat', pipe_cat, cat_cols),\n",
    "#                                                     ('num', pipe_num, num_cols),\n",
    "#                                                     ('high_dim', pipe_high_dim, high_dim_cols)])\n",
    "#\n",
    "#                     # Criando o pipeline final\n",
    "#                     pipe = Pipeline([('transformer', transformer),\n",
    "#                                     ('model', model)])\n",
    "#\n",
    "#                     # Executando o cross validation\n",
    "#                     cross_val_scores = cross_val_score(pipe, x_treino, y_treino, cv=kf, scoring='neg_log_loss')\n",
    "#\n",
    "#                     # Calculando a média das métricas\n",
    "#                     mean_score = cross_val_scores.mean()         \n",
    "#\n",
    "#                     # Salvando a métrica da folder 1\n",
    "#                     mlflow.log_metric('log_loss_fold_1', cross_val_scores[0])\n",
    "#\n",
    "#                     # Salvando a métrica da folder 2\n",
    "#                     mlflow.log_metric('log_loss_fold_2', cross_val_scores[1])\n",
    "#\n",
    "#                     # Salvando a métrica da folder 3\n",
    "#                     mlflow.log_metric('log_loss_fold_3', cross_val_scores[2])\n",
    "#\n",
    "#                     # Salvando a métrica da folder 4\n",
    "#                     mlflow.log_metric('log_loss_fold_4', cross_val_scores[3])\n",
    "#\n",
    "#                     # Salvando a métrica da folder 5\n",
    "#                     mlflow.log_metric('log_loss_fold_5', cross_val_scores[4])\n",
    "#\n",
    "#                     # Salvando as métricas\n",
    "#                     mlflow.log_metric('log_loss_mean', mean_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Iniciando os experimentos sem transformers e com class_weight\n",
    "#for tag, model in dict_models_tree_based_cw.items():\n",
    "#    for tag_encoder, encoder in dict_encoders.items():\n",
    "#            \n",
    "#            # Gerando a tag de identificação do modelo\n",
    "#            nome_modelo = f'{tag}_CW_{tag_encoder}'\n",
    "#            \n",
    "#            with mlflow.start_run(run_name=nome_modelo):\n",
    "#                 \n",
    "#                 # Criando os pipeline com os transformers\n",
    "#                 pipe_cat = Pipeline([('encoder', encoder)])\n",
    "#                 pipe_high_dim = Pipeline([('encoder', ce.CountEncoder())])\n",
    "#                 \n",
    "#                 # Criando o transformador\n",
    "#                 transformer = ColumnTransformer([('cat', pipe_cat, cat_cols),\n",
    "#                                                 ('high_dim', pipe_high_dim, high_dim_cols)])\n",
    "#                 \n",
    "#                 # Criando o pipeline final\n",
    "#                 pipe = Pipeline([('transformer', transformer),\n",
    "#                                 ('model', model)])\n",
    "#                 \n",
    "#                 # Executando o cross validation\n",
    "#                 cross_val_scores = cross_val_score(pipe, x_treino, y_treino, cv=kf, scoring='neg_log_loss')\n",
    "#                 \n",
    "#                 # Calculando a média das métricas\n",
    "#                 mean_score = cross_val_scores.mean()         \n",
    "#                 \n",
    "#                 # Salvando a métrica da folder 1\n",
    "#                 mlflow.log_metric('log_loss_fold_1', cross_val_scores[0])\n",
    "#                 \n",
    "#                 # Salvando a métrica da folder 2\n",
    "#                 mlflow.log_metric('log_loss_fold_2', cross_val_scores[1])\n",
    "#                \n",
    "#                 # Salvando a métrica da folder 3\n",
    "#                 mlflow.log_metric('log_loss_fold_3', cross_val_scores[2])\n",
    "#                \n",
    "#                 # Salvando a métrica da folder 4\n",
    "#                 mlflow.log_metric('log_loss_fold_4', cross_val_scores[3])\n",
    "#                \n",
    "#                 # Salvando a métrica da folder 5\n",
    "#                 mlflow.log_metric('log_loss_fold_5', cross_val_scores[4])\n",
    "#                 \n",
    "#                 # Salvando as métricas\n",
    "#                 mlflow.log_metric('log_loss_mean', mean_score)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Iniciando os experimentos sem transformers e sem class_weight\n",
    "#for tag, model in dict_models_tree_based_no_cw.items():\n",
    "#    for tag_encoder, encoder in dict_encoders.items():\n",
    "#            \n",
    "#            # Gerando a tag de identificação do modelo\n",
    "#            nome_modelo = f'{tag}_NO_CW_{tag_encoder}'\n",
    "#            \n",
    "#            with mlflow.start_run(run_name=nome_modelo):\n",
    "#                 \n",
    "#                 # Criando os pipeline com os transformers\n",
    "#                 pipe_cat = Pipeline([('encoder', encoder)])\n",
    "#                 pipe_high_dim = Pipeline([('encoder', ce.CountEncoder())])\n",
    "#                 \n",
    "#                 # Criando o transformador\n",
    "#                 transformer = ColumnTransformer([('cat', pipe_cat, cat_cols),\n",
    "#                                                 ('high_dim', pipe_high_dim, high_dim_cols)])\n",
    "#                 \n",
    "#                 # Criando o pipeline final\n",
    "#                 pipe = Pipeline([('transformer', transformer),\n",
    "#                                 ('model', model)])\n",
    "#                 \n",
    "#                 # Executando o cross validation\n",
    "#                 cross_val_scores = cross_val_score(pipe, x_treino, y_treino, cv=kf, scoring='neg_log_loss')\n",
    "#                 \n",
    "#                 # Calculando a média das métricas\n",
    "#                 mean_score = cross_val_scores.mean()         \n",
    "#                 \n",
    "#                 # Salvando a métrica da folder 1\n",
    "#                 mlflow.log_metric('log_loss_fold_1', cross_val_scores[0])\n",
    "#                 \n",
    "#                 # Salvando a métrica da folder 2\n",
    "#                 mlflow.log_metric('log_loss_fold_2', cross_val_scores[1])\n",
    "#                \n",
    "#                 # Salvando a métrica da folder 3\n",
    "#                 mlflow.log_metric('log_loss_fold_3', cross_val_scores[2])\n",
    "#                \n",
    "#                 # Salvando a métrica da folder 4\n",
    "#                 mlflow.log_metric('log_loss_fold_4', cross_val_scores[3])\n",
    "#                \n",
    "#                 # Salvando a métrica da folder 5\n",
    "#                 mlflow.log_metric('log_loss_fold_5', cross_val_scores[4])\n",
    "#                 \n",
    "#                 # Salvando as métricas\n",
    "#                 mlflow.log_metric('log_loss_mean', mean_score)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Especificando as colunas para retornar\n",
    "colunas = ['tags.mlflow.runName', 'metrics.log_loss_mean']\n",
    "\n",
    "# Buscando os melhores modelos\n",
    "mlflow.search_runs(order_by=['metrics.log_loss_mean DESC'], max_results=15)[colunas]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os modelos sem Class_Weight foram os melhores classificadores. Dentre os \n",
    "melhores, é perceptível a baixa diferença entre os modelos. Dado a isso, iremos\n",
    "selecionar os dois melhores modelos para a tunagem de hiperparâmetros, que são \n",
    "a Regressão Logística e o CatBoost. Os dois serão testados em um ensemble.\n",
    "\n",
    "Como a natureza da solução exige respostas rápidas, a Regressão Logística terá \n",
    "preferência sobre o CatBoost e o Ensemble de ambos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Criando função para tunar o modelo\n",
    "#def objective(trial):\n",
    "#\n",
    "#    params = {\n",
    "#        'C': trial.suggest_float('C', 1e-4, 1e+4, log=True),\n",
    "#        'penalty': trial.suggest_categorical('penalty', [None, 'l2']),\n",
    "#        'solver': trial.suggest_categorical('solver', ['lbfgs', 'saga', 'newton-cholesky']),\n",
    "#        'max_iter': trial.suggest_int('max_iter', 50, 1000),\n",
    "#        'fit_intercept': trial.suggest_categorical('fit_intercept', [True, False]),\n",
    "#        'class_weight': trial.suggest_categorical('class_weight', [None, 'balanced']),\n",
    "#        'random_state': 200\n",
    "#    }\n",
    "#    \n",
    "#    # Criando os pipeline com os transformers\n",
    "#    pipe_cat = Pipeline([('encoder', ce.BinaryEncoder())])\n",
    "#    pipe_high_dim = Pipeline([('encoder', ce.CountEncoder())])\n",
    "#    pipe_num = Pipeline([('scaler', StandardScaler())])\n",
    "#    \n",
    "#    # Criando o transformador\n",
    "#    transformer = ColumnTransformer([('cat', pipe_cat, cat_cols),\n",
    "#                                    ('num', pipe_num, num_cols),\n",
    "#                                    ('high_dim', pipe_high_dim, high_dim_cols)])\n",
    "#    \n",
    "#    # Criando o pipeline final\n",
    "#    pipe = Pipeline([('transformer', transformer),\n",
    "#                    ('model', LogisticRegression(**params))])\n",
    "#\n",
    "#    # Treinando o modelo com os dados de treino\n",
    "#    pipe.fit(x_treino, y_treino)\n",
    "#   \n",
    "#    logloss = log_loss(y_dev, pipe.predict_proba(x_dev))\n",
    "#    \n",
    "#    return logloss\n",
    "#\n",
    "## Criando o estudo de otimização\n",
    "#study = optuna.create_study(direction = 'minimize')\n",
    "#study.optimize(objective, n_trials = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O resultado do hiperparâmetro otimizado pode ser diferente a cada execução.\n",
    "# Resultado atingido: 0.23\n",
    "\n",
    "# Checando os melhores parâmetros\n",
    "#study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Specifying the columns using strings is only supported for pandas DataFrames",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/sisu_analysis/venv/lib/python3.10/site-packages/sklearn/utils/__init__.py:424\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 424\u001b[0m     all_columns \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 21\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Criando o pipeline final\u001b[39;00m\n\u001b[1;32m     12\u001b[0m best_lr \u001b[38;5;241m=\u001b[39m Pipeline([(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer\u001b[39m\u001b[38;5;124m'\u001b[39m, transformer),\n\u001b[1;32m     13\u001b[0m                 (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m, LogisticRegression(C\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.08250109742237544\u001b[39m,\n\u001b[1;32m     14\u001b[0m                                              penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml2\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m                                              class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m                                              random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m))])\n\u001b[0;32m---> 21\u001b[0m \u001b[43mbest_lr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_treino\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_treino\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/sisu_analysis/venv/lib/python3.10/site-packages/sklearn/pipeline.py:401\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \n\u001b[1;32m    377\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    400\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m--> 401\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/sisu_analysis/venv/lib/python3.10/site-packages/sklearn/pipeline.py:359\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    357\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m~/Documents/sisu_analysis/venv/lib/python3.10/site-packages/joblib/memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/sisu_analysis/venv/lib/python3.10/site-packages/sklearn/pipeline.py:893\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 893\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    895\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/Documents/sisu_analysis/venv/lib/python3.10/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/sisu_analysis/venv/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:724\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_transformers()\n\u001b[0;32m--> 724\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_column_callables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_remainder(X)\n\u001b[1;32m    727\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_transform(X, y, _fit_transform_one)\n",
      "File \u001b[0;32m~/Documents/sisu_analysis/venv/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:426\u001b[0m, in \u001b[0;36mColumnTransformer._validate_column_callables\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    424\u001b[0m         columns \u001b[38;5;241m=\u001b[39m columns(X)\n\u001b[1;32m    425\u001b[0m     all_columns\u001b[38;5;241m.\u001b[39mappend(columns)\n\u001b[0;32m--> 426\u001b[0m     transformer_to_input_indices[name] \u001b[38;5;241m=\u001b[39m \u001b[43m_get_column_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_columns \u001b[38;5;241m=\u001b[39m all_columns\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer_to_input_indices \u001b[38;5;241m=\u001b[39m transformer_to_input_indices\n",
      "File \u001b[0;32m~/Documents/sisu_analysis/venv/lib/python3.10/site-packages/sklearn/utils/__init__.py:426\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    424\u001b[0m     all_columns \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpecifying the columns using strings is only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported for pandas DataFrames\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m     )\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    431\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [key]\n",
      "\u001b[0;31mValueError\u001b[0m: Specifying the columns using strings is only supported for pandas DataFrames"
     ]
    }
   ],
   "source": [
    "# Criando os pipeline com os transformers\n",
    "pipe_cat = Pipeline([('encoder', ce.BinaryEncoder())])\n",
    "pipe_high_dim = Pipeline([('encoder', ce.CountEncoder())])\n",
    "pipe_num = Pipeline([('scaler', StandardScaler())])\n",
    "\n",
    "# Criando o transformador\n",
    "transformer = ColumnTransformer([('cat', pipe_cat, cat_cols),\n",
    "                                ('num', pipe_num, num_cols),\n",
    "                                ('high_dim', pipe_high_dim, high_dim_cols)])\n",
    "\n",
    "# Criando o pipeline final\n",
    "best_lr = Pipeline([('transformer', transformer),\n",
    "                ('model', LogisticRegression(C=0.08250109742237544,\n",
    "                                             penalty='l2',\n",
    "                                             solver='newton-cholesky',\n",
    "                                             max_iter=244,\n",
    "                                             fit_intercept=False,\n",
    "                                             class_weight=None,\n",
    "                                             random_state=200))])\n",
    "\n",
    "best_lr.fit(x_treino.values, y_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Criando função para tunar o modelo\n",
    "#def objective(trial):\n",
    "#\n",
    "#    params = {\n",
    "#        'objective': 'Logloss',\n",
    "#        'eval_metric': 'Logloss',\n",
    "#        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1, 10),\n",
    "#        'depth': trial.suggest_int('depth', 3, 10),\n",
    "#        'min_child_samples': trial.suggest_int('min_child_samples', 1, 20),\n",
    "#        'subsample': trial.suggest_float('subsample', 0.5, 1),\n",
    "#        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 1),\n",
    "#        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10),\n",
    "#        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "#        'random_state': 200\n",
    "#    }\n",
    "#    \n",
    "#    pipe_cat = Pipeline([('encoder', ce.GrayEncoder())])\n",
    "#    pipe_high_dim = Pipeline([('encoder', ce.CountEncoder())])\n",
    "#\n",
    "#    # Criando o transformador\n",
    "#    transformer = ColumnTransformer([('cat', pipe_cat, cat_cols),\n",
    "#                                    ('high_dim', pipe_high_dim, high_dim_cols)])\n",
    "#\n",
    "#    # Criando o pipeline final\n",
    "#    pipe = Pipeline([('transformer', transformer),\n",
    "#                    ('rf', CatBoostClassifier(**params))])\n",
    "#\n",
    "#    # Treinando o modelo com os dados de treino\n",
    "#    pipe.fit(x_treino, y_treino)\n",
    "#   \n",
    "#    logloss = log_loss(y_dev, pipe.predict_proba(x_dev))\n",
    "#    \n",
    "#    return logloss\n",
    "#\n",
    "## Criando o estudo de otimização\n",
    "#study = optuna.create_study(direction = 'minimize')\n",
    "#study.optimize(objective, n_trials = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não foi registrado um aumento significativa com a tunagem, por isso, o modelo\n",
    "será o padrão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testando e retreianando o modelo com todos os dados\n",
    "\n",
    "Como o Catboost não obteve um aumento significativo com a tunagem, iremos\n",
    "seguir apenas com a Regressão Logística. Além disso, por não necessitar de\n",
    "calibração de probabilidade, essa parte será pulada. Entretando, como os dados \n",
    "para calibração já foram divididos, iremos utiliza-los para avaliação extra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliando a melhor Regressão Logística nos dados de teste\n",
    "log_loss(y_teste, best_lr.predict_proba(x_teste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliando a melhor Regressão Logística nos dados de calibração\n",
    "log_loss(y_calib, best_lr.predict_proba(x_calib))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreinando o modelo com todos os dados\n",
    "best_lr.fit(x.values, y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando o modelo\n",
    "joblib.dump(best_lr, '../models/best_lr.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvando parte dos dados com informações relevantes para o Web App\n",
    "Por fim, a parte dos dados que contém informações relevantes para o Web App\n",
    "será salva em um arquivo .db para ser consumido pelo app. Colunas referentes as\n",
    "notas dos candidatos serão removidas, pois serão fornecidas pelo usuário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionando uma coluna extra na lista\n",
    "colunas_para_buscar.append('MOD_CONCORRENCIA')\n",
    "\n",
    "# Selecionando as colunas\n",
    "dados_to_save = dados_sisu_full[colunas_para_buscar]\n",
    "dados_to_save = dados_to_save.drop(['NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', 'NOTA_R', 'NOTA_CANDIDATO'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando os dados no formato .db\n",
    "dados_to_save.to_sql('dados_to_web', 'sqlite:///../data/processed/dados_to_web.db', index=False, if_exists='replace')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
