{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/29 11:15:46 WARN Utils: Your hostname, daniel-VJFE43F11X-XXXXXX resolves to a loopback address: 127.0.1.1; using 192.168.0.157 instead (on interface wlo1)\n",
      "23/11/29 11:15:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/11/29 11:15:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/29 11:15:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Caminho para o jar do sqlite\n",
    "caminho_jar_sqlite = \"../drivers/sqlite-jdbc-3.44.1.0.jar\"\n",
    "\n",
    "# Criando uma sessão spark\n",
    "spark = SparkSession.builder.appName(\"modelling\").config(\"spark.jars\", caminho_jar_sqlite).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lendo o arquivo csv\n",
    "dados = spark.read.csv(\"../data/raw/chamada_regular_sisu_2022_1.csv\", header=True, sep=\"|\", encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/29 11:16:01 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----+---------------+----------+--------------------+---------+------+-------------+--------------------+---------+----------------+------------+--------------------+-----------+-------+----------------+-------+---------------------+------------------+---------------------+----------------+------+-------+-------+------+------+-------------+--------------+--------------+-------------+-------------+------------+-------------+--------------+--------------------+----+---------------+------------+-------------------+-----+------+-------+-------+------+------+---------------+----------------+----------------+---------------+---------------+--------------+----------+-------------+--------+--------------+\n",
      "| ANO|EDICAO|ETAPA|       DS_ETAPA|CODIGO_IES|            NOME_IES|SIGLA_IES|UF_IES|CODIGO_CAMPUS|         NOME_CAMPUS|UF_CAMPUS|MUNICIPIO_CAMPUS|CODIGO_CURSO|          NOME_CURSO|       GRAU|  TURNO|DS_PERIODICIDADE|TP_COTA|TIPO_MOD_CONCORRENCIA|  MOD_CONCORRENCIA|QT_VAGAS_CONCORRENCIA|PERCENTUAL_BONUS|PESO_L|PESO_CH|PESO_CN|PESO_M|PESO_R|NOTA_MINIMA_L|NOTA_MINIMA_CH|NOTA_MINIMA_CN|NOTA_MINIMA_M|NOTA_MINIMA_R|MEDIA_MINIMA|          CPF|INSCRICAO_ENEM|            INSCRITO|SEXO|DATA_NASCIMENTO|UF_CANDIDATO|MUNICIPIO_CANDIDATO|OPCAO|NOTA_L|NOTA_CH|NOTA_CN|NOTA_M|NOTA_R|NOTA_L_COM_PESO|NOTA_CH_COM_PESO|NOTA_CN_COM_PESO|NOTA_M_COM_PESO|NOTA_R_COM_PESO|NOTA_CANDIDATO|NOTA_CORTE|CLASSIFICACAO|APROVADO|     MATRICULA|\n",
      "+----+------+-----+---------------+----------+--------------------+---------+------+-------------+--------------------+---------+----------------+------------+--------------------+-----------+-------+----------------+-------+---------------------+------------------+---------------------+----------------+------+-------+-------+------+------+-------------+--------------+--------------+-------------+-------------+------------+-------------+--------------+--------------------+----+---------------+------------+-------------------+-----+------+-------+-------+------+------+---------------+----------------+----------------+---------------+---------------+--------------+----------+-------------+--------+--------------+\n",
      "|2022|     1|    4|CHAMADA REGULAR|       593|CENTRO FEDERAL DE...| CEFET/RJ|    RJ|         1663|CEFET-RJ - MARIA ...|       RJ|  Rio de Janeiro|     1441998|SISTEMAS DE INFOR...|Bacharelado|Noturno|       Semestral|   NULL|                    A|Ampla concorrência|                   15|            NULL|     1|      1|      2|     4|     3|        453,8|         444,7|         453,3|        438,4|          300|      418,04|XXX.015687-XX|  211XXXXXX424|ABRAAO CARDOSO DO...|   M|     20/08/1979|          RJ|       Belford Roxo|    2| 564,8|  600,5|  616,9| 711,1|   580|          564,8|           600,5|          1233,8|         2844,4|           1740|        634,86|    718,99|           61|       N|      PENDENTE|\n",
      "|2022|     1|    4|CHAMADA REGULAR|       593|CENTRO FEDERAL DE...| CEFET/RJ|    RJ|         1663|CEFET-RJ - MARIA ...|       RJ|  Rio de Janeiro|     1441998|SISTEMAS DE INFOR...|Bacharelado|Noturno|       Semestral|   NULL|                    A|Ampla concorrência|                   15|            NULL|     1|      1|      2|     4|     3|        453,8|         444,7|         453,3|        438,4|          300|      418,04|XXX.781997-XX|  211XXXXXX857|    ADHAM GREG DIEHL|   M|     01/04/1986|          RJ|     Rio de Janeiro|    1|   650|  669,9|  618,3| 698,7|   640|            650|           669,9|          1236,6|         2794,8|           1920|        661,03|    718,99|           45|       N|      PENDENTE|\n",
      "|2022|     1|    4|CHAMADA REGULAR|       593|CENTRO FEDERAL DE...| CEFET/RJ|    RJ|         1663|CEFET-RJ - MARIA ...|       RJ|  Rio de Janeiro|     1441998|SISTEMAS DE INFOR...|Bacharelado|Noturno|       Semestral|   NULL|                    A|Ampla concorrência|                   15|            NULL|     1|      1|      2|     4|     3|        453,8|         444,7|         453,3|        438,4|          300|      418,04|XXX.040767-XX|  211XXXXXX696|ALEX SANDRO DA CO...|   M|     31/01/2004|          RJ|    Duque de Caxias|    2| 618,7|  602,2|  666,3|   679|   880|          618,7|           602,2|          1332,6|           2716|           2640|        719,05|    718,99|           14|       S|NÃO COMPARECEU|\n",
      "|2022|     1|    4|CHAMADA REGULAR|       593|CENTRO FEDERAL DE...| CEFET/RJ|    RJ|         1663|CEFET-RJ - MARIA ...|       RJ|  Rio de Janeiro|     1441998|SISTEMAS DE INFOR...|Bacharelado|Noturno|       Semestral|   NULL|                    A|Ampla concorrência|                   15|            NULL|     1|      1|      2|     4|     3|        453,8|         444,7|         453,3|        438,4|          300|      418,04|XXX.630087-XX|  211XXXXXX981|  ANDRE LOPES BIONDE|   M|     06/05/2000|          RJ|     Rio de Janeiro|    2| 543,6|  514,6|  455,5| 662,9|   520|          543,6|           514,6|             911|         2651,6|           1560|        561,89|    718,99|           90|       N|      PENDENTE|\n",
      "|2022|     1|    4|CHAMADA REGULAR|       593|CENTRO FEDERAL DE...| CEFET/RJ|    RJ|         1663|CEFET-RJ - MARIA ...|       RJ|  Rio de Janeiro|     1441998|SISTEMAS DE INFOR...|Bacharelado|Noturno|       Semestral|   NULL|                    A|Ampla concorrência|                   15|            NULL|     1|      1|      2|     4|     3|        453,8|         444,7|         453,3|        438,4|          300|      418,04|XXX.802497-XX|  211XXXXXX762|ANDRE LUIZ DE SOU...|   M|     20/09/1997|          RJ|          Nilópolis|    2| 637,8|  741,4|  595,4| 671,7|   640|          637,8|           741,4|          1190,8|         2686,8|           1920|        652,44|    718,99|           51|       N|      PENDENTE|\n",
      "+----+------+-----+---------------+----------+--------------------+---------+------+-------------+--------------------+---------+----------------+------------+--------------------+-----------+-------+----------------+-------+---------------------+------------------+---------------------+----------------+------+-------+-------+------+------+-------------+--------------+--------------+-------------+-------------+------------+-------------+--------------+--------------------+----+---------------+------------+-------------------+-----+------+-------+-------+------+------+---------------+----------------+----------------+---------------+---------------+--------------+----------+-------------+--------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Observando o estado inicial dos dados\n",
    "dados.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ANO: string (nullable = true)\n",
      " |-- EDICAO: string (nullable = true)\n",
      " |-- ETAPA: string (nullable = true)\n",
      " |-- DS_ETAPA: string (nullable = true)\n",
      " |-- CODIGO_IES: string (nullable = true)\n",
      " |-- NOME_IES: string (nullable = true)\n",
      " |-- SIGLA_IES: string (nullable = true)\n",
      " |-- UF_IES: string (nullable = true)\n",
      " |-- CODIGO_CAMPUS: string (nullable = true)\n",
      " |-- NOME_CAMPUS: string (nullable = true)\n",
      " |-- UF_CAMPUS: string (nullable = true)\n",
      " |-- MUNICIPIO_CAMPUS: string (nullable = true)\n",
      " |-- CODIGO_CURSO: string (nullable = true)\n",
      " |-- NOME_CURSO: string (nullable = true)\n",
      " |-- GRAU: string (nullable = true)\n",
      " |-- TURNO: string (nullable = true)\n",
      " |-- DS_PERIODICIDADE: string (nullable = true)\n",
      " |-- TP_COTA: string (nullable = true)\n",
      " |-- TIPO_MOD_CONCORRENCIA: string (nullable = true)\n",
      " |-- MOD_CONCORRENCIA: string (nullable = true)\n",
      " |-- QT_VAGAS_CONCORRENCIA: string (nullable = true)\n",
      " |-- PERCENTUAL_BONUS: string (nullable = true)\n",
      " |-- PESO_L: string (nullable = true)\n",
      " |-- PESO_CH: string (nullable = true)\n",
      " |-- PESO_CN: string (nullable = true)\n",
      " |-- PESO_M: string (nullable = true)\n",
      " |-- PESO_R: string (nullable = true)\n",
      " |-- NOTA_MINIMA_L: string (nullable = true)\n",
      " |-- NOTA_MINIMA_CH: string (nullable = true)\n",
      " |-- NOTA_MINIMA_CN: string (nullable = true)\n",
      " |-- NOTA_MINIMA_M: string (nullable = true)\n",
      " |-- NOTA_MINIMA_R: string (nullable = true)\n",
      " |-- MEDIA_MINIMA: string (nullable = true)\n",
      " |-- CPF: string (nullable = true)\n",
      " |-- INSCRICAO_ENEM: string (nullable = true)\n",
      " |-- INSCRITO: string (nullable = true)\n",
      " |-- SEXO: string (nullable = true)\n",
      " |-- DATA_NASCIMENTO: string (nullable = true)\n",
      " |-- UF_CANDIDATO: string (nullable = true)\n",
      " |-- MUNICIPIO_CANDIDATO: string (nullable = true)\n",
      " |-- OPCAO: string (nullable = true)\n",
      " |-- NOTA_L: string (nullable = true)\n",
      " |-- NOTA_CH: string (nullable = true)\n",
      " |-- NOTA_CN: string (nullable = true)\n",
      " |-- NOTA_M: string (nullable = true)\n",
      " |-- NOTA_R: string (nullable = true)\n",
      " |-- NOTA_L_COM_PESO: string (nullable = true)\n",
      " |-- NOTA_CH_COM_PESO: string (nullable = true)\n",
      " |-- NOTA_CN_COM_PESO: string (nullable = true)\n",
      " |-- NOTA_M_COM_PESO: string (nullable = true)\n",
      " |-- NOTA_R_COM_PESO: string (nullable = true)\n",
      " |-- NOTA_CANDIDATO: string (nullable = true)\n",
      " |-- NOTA_CORTE: string (nullable = true)\n",
      " |-- CLASSIFICACAO: string (nullable = true)\n",
      " |-- APROVADO: string (nullable = true)\n",
      " |-- MATRICULA: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checando o schema dos dados\n",
    "dados.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permitindo usar SQL nos dados\n",
    "dados.createOrReplaceTempView(\"dados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dados aparentam estar em um bom estados, onde as únicas alterações exigidas \n",
    "são a mudança do tipo de dado de algumas colunas e a substituição do \".\" por \",\"\n",
    "em todos os valores de notas, além da mudança de tipo dos mesmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando as correções\n",
    "dados_transformados = spark.sql(\"SELECT CAST(ANO AS INT) AS ANO, \\\n",
    "                                CAST(EDICAO AS INT) AS EDICAO, \\\n",
    "                                CAST(ETAPA AS INT) AS ETAPA, \\\n",
    "                                DS_ETAPA, \\\n",
    "                                CAST(CODIGO_IES AS INT) AS CODIGO_IES, \\\n",
    "                                NOME_IES, \\\n",
    "                                SIGLA_IES, \\\n",
    "                                UF_IES, \\\n",
    "                                CAST(CODIGO_CAMPUS AS INT) AS CODIGO_CAMPUS, \\\n",
    "                                NOME_CAMPUS, \\\n",
    "                                UF_CAMPUS, \\\n",
    "                                MUNICIPIO_CAMPUS, \\\n",
    "                                CAST(CODIGO_CURSO AS INT) AS CODIGO_CURSO, \\\n",
    "                                NOME_CURSO, \\\n",
    "                                GRAU, \\\n",
    "                                TURNO, \\\n",
    "                                DS_PERIODICIDADE, \\\n",
    "                                CASE WHEN TP_COTA IS NULL THEN 'Não se aplica' ELSE TP_COTA END AS TP_COTA, \\\n",
    "                                TIPO_MOD_CONCORRENCIA, \\\n",
    "                                MOD_CONCORRENCIA, \\\n",
    "                                CAST(QT_VAGAS_CONCORRENCIA AS INT) AS QT_VAGAS_CONCORRENCIA, \\\n",
    "                                CAST(PERCENTUAL_BONUS AS INT) AS PERCENTUAL_BONUS, \\\n",
    "                                PESO_L, \\\n",
    "                                PESO_CH, \\\n",
    "                                PESO_CN, \\\n",
    "                                PESO_M, \\\n",
    "                                PESO_R, \\\n",
    "                                CAST(REPLACE(NOTA_MINIMA_L, ',', '.') AS FLOAT) AS NOTA_MINIMA_L, \\\n",
    "                                CAST(REPLACE(NOTA_MINIMA_CH, ',', '.') AS FLOAT) AS NOTA_MINIMA_CH, \\\n",
    "                                CAST(REPLACE(NOTA_MINIMA_CN, ',', '.') AS FLOAT) AS NOTA_MINIMA_CN, \\\n",
    "                                CAST(REPLACE(NOTA_MINIMA_M, ',', '.') AS FLOAT) AS NOTA_MINIMA_M, \\\n",
    "                                CAST(REPLACE(NOTA_MINIMA_R, ',', '.') AS FLOAT) AS NOTA_MINIMA_R, \\\n",
    "                                CAST(REPLACE(MEDIA_MINIMA, ',', '.') AS FLOAT) AS MEDIA_MINIMA, \\\n",
    "                                CPF, \\\n",
    "                                INSCRICAO_ENEM, \\\n",
    "                                INSCRITO, \\\n",
    "                                SEXO, \\\n",
    "                                CAST(TO_DATE(DATA_NASCIMENTO, 'dd/MM/yyyy') AS DATE) AS DATA_NASCIMENTO, \\\n",
    "                                UF_CANDIDATO, \\\n",
    "                                MUNICIPIO_CANDIDATO, \\\n",
    "                                OPCAO, \\\n",
    "                                CAST(REPLACE(NOTA_L, ',', '.') AS FLOAT) AS NOTA_L, \\\n",
    "                                CAST(REPLACE(NOTA_CH, ',', '.') AS FLOAT) AS NOTA_CH, \\\n",
    "                                CAST(REPLACE(NOTA_CN, ',', '.') AS FLOAT) AS NOTA_CN, \\\n",
    "                                CAST(REPLACE(NOTA_M, ',', '.') AS FLOAT) AS NOTA_M, \\\n",
    "                                CAST(REPLACE(NOTA_R, ',', '.') AS FLOAT) AS NOTA_R, \\\n",
    "                                CAST(REPLACE(NOTA_L_COM_PESO, ',', '.') AS FLOAT) AS NOTA_L_COM_PESO, \\\n",
    "                                CAST(REPLACE(NOTA_CH_COM_PESO, ',', '.') AS FLOAT) AS NOTA_CH_COM_PESO, \\\n",
    "                                CAST(REPLACE(NOTA_CN_COM_PESO, ',', '.') AS FLOAT) AS NOTA_CN_COM_PESO, \\\n",
    "                                CAST(REPLACE(NOTA_M_COM_PESO, ',', '.') AS FLOAT) AS NOTA_M_COM_PESO, \\\n",
    "                                CAST(REPLACE(NOTA_R_COM_PESO, ',', '.') AS FLOAT) AS NOTA_R_COM_PESO, \\\n",
    "                                CAST(REPLACE(NOTA_CANDIDATO, ',', '.') AS FLOAT) AS NOTA_CANDIDATO, \\\n",
    "                                CAST(REPLACE(NOTA_CORTE, ',', '.') AS FLOAT) AS NOTA_CORTE, \\\n",
    "                                CLASSIFICACAO, \\\n",
    "                                APROVADO, \\\n",
    "                                MATRICULA \\\n",
    "                                FROM dados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o43.save.\n: java.sql.SQLException: No suitable driver\n\tat java.sql/java.sql.DriverManager.getDriver(DriverManager.java:298)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:109)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:109)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:246)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:250)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/home/daniel/Documents/sisu_analysis/notebooks/data_modelling.ipynb Célula 9\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/daniel/Documents/sisu_analysis/notebooks/data_modelling.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Salvando os dados transformados n formato de database\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/daniel/Documents/sisu_analysis/notebooks/data_modelling.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dados_transformados\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mformat(\u001b[39m\"\u001b[39;49m\u001b[39mjdbc\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/daniel/Documents/sisu_analysis/notebooks/data_modelling.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39murl\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mjdbc:sqlite:\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mformat(\u001b[39m\"\u001b[39;49m\u001b[39m../data/processed/sisu.db\u001b[39;49m\u001b[39m\"\u001b[39;49m)) \\\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/daniel/Documents/sisu_analysis/notebooks/data_modelling.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mdbtable\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mdata_sisu\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/daniel/Documents/sisu_analysis/notebooks/data_modelling.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m.\u001b[39;49mmode(\u001b[39m\"\u001b[39;49m\u001b[39moverwrite\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/daniel/Documents/sisu_analysis/notebooks/data_modelling.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m.\u001b[39;49msave()\n",
      "File \u001b[0;32m~/Documents/sisu_analysis/venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1461\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mformat\u001b[39m)\n\u001b[1;32m   1460\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1461\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49msave()\n\u001b[1;32m   1462\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1463\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jwrite\u001b[39m.\u001b[39msave(path)\n",
      "File \u001b[0;32m~/Documents/sisu_analysis/venv/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/sisu_analysis/venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Documents/sisu_analysis/venv/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o43.save.\n: java.sql.SQLException: No suitable driver\n\tat java.sql/java.sql.DriverManager.getDriver(DriverManager.java:298)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:109)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:109)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:246)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:250)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "# Salvando os dados transformados n formato de database\n",
    "dados_transformados.write.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:sqlite:{}\".format(\"../data/processed/sisu.db\")) \\\n",
    "    .option(\"dbtable\", \"data_sisu\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
